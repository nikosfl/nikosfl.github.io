---
layout: post
title: Speaker Role Recognition
subtitle: and how to combine it with other speech processing tasks
use-site-title: true
permalink: research/srr
involved: 2018-2022
---

Individuals assume distinct roles in different situations throughout their lives and people who consistently adopt particular roles develop specific commonalities in behavior. As a result, roles can be defined in terms of observable tendencies and behavioral patterns that can be manifest through a wide range of modalities during a conversational interaction. For instance, an interviewer
is expected to use more interrogative words than the interviewee and a teacher is likely to speak in a more didactic style than the student.

<p align="center">
  <img src="/img/roles_eg.png" width="400">  
</p>
<em><font size="-1">
Examples of conversational interactions where speakers assume distinct roles.  <br>
Images from the Noun Project; creators: Nubaia Karim Barsha, Gan Khun Lay, Arafat Uddin, Llisole, ProSymbols
</font></em>

Speaker role recognition is the task of assigning a role label in a speech segment where a single speaker is active, through computational models that capture such behavioral characteristics. 
The approaches that tackle this problem depend on successful pre-processing steps applied on the recorded conversation, such as speaker segmentation and clustering or automatic speech recognition (ASR), something that inevitably leads to error propagation. 
At the same time, accurate role information can provide valuable contextual cues for the aforementioned speech processing tasks.

What I am trying to do in this research thread, which is the subject of my [PhD dissertation](https://nikosfl.github.io/work/thesis/dissertation_NF_2022.pdf), supervised by [Professor Shrikanth Narayanan](https://sail.usc.edu/people/shri.php), is combine speaker role recognition with other speech processing modules, in order to alleviate the problem of error propagation and, at the same time, improve the performance of the modules it is combined with. Since, in a real-world scenario, the role recognizer gets the textual information from an ASR system, [here](/work/papers/2019_ICASSP_Role_Specific_ASR.pdf) we propose building role-specific ASR outputs through lattice rescoring, exploiting the rich information in the generic decoding lattice before pruning it. [In this paper](/work/papers/2018_SLT_RASR.pdf) we propose an end-to-end rich transcription system which performs simultaneously the tasks of ASR, diarization, and role recognition for the case of dyadic interactions. [Here](/work/papers/2018_IS_SpeakerClustering.pdf) we combine a speaker role recognizer with a traditional agglomerative speaker clustering module, exploiting information from both the acoustic and linguistic modalities. Finally, [those](/work/papers/2020_ODYSSEY_Linguistically_Diarization_Roles.pdf) [papers](https://nikosfl.github.io/work/papers/2022_IS_MultimodalClusteringRoleInducedConstraints.pdf) present a way to utilize in-domain information about speaker roles in order to improve the performance of a traditional speaker diarization system, answering the question ``which role spoke when".

<!-- last updated: 2025-04-21-->
