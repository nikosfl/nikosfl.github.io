---
layout: page
title: Coursework
use-site-title: true
---

<script type="text/javascript">
function showOrHide(id) 
{
    var div = document.getElementById(id);
    if (div.style.display == "block") 
    {
        div.style.display = "none";
    }
    else 
    {
        div.style.display = "block";
    }
}
</script>

## University of Southern California
### Electrical Engineering
* EE 682: Law and Intellectual Property for Engineers (Fall 2020)  
<a href="javascript:showOrHide('syl_ee682');">Outline</a>
<div style="display:none" id="syl_ee682">
<ol class="listing-grey" style="margin-left: 4%;">
  <li>American legal system; Intellectual property</li>
  <li>Torts I: Intentional harm and negligence</li>
  <li>Torts II: Strict liability; Patent and copyright infringement</li>
  <li>Criminal law and procedure; Theft of trade secrets</li>
  <li>Contracts I: Making and breaking promises</li>
  <li>Contracts II: Third-party liability; Assignments and licenses</li>
  <li>Law of property; Estates; Transfers</li>
  <li>Civil procedure and federal courts; Standing</li>
  <li>Constitutional law; Congressional authority; State vs. federal law</li>
  <li>Evidence; Standards of proof; Relevance</li>
  <li>Corporations; Cyberlaw</li>
  <li>Patents; Drafting claims</li>
  <li>Copyright; Trade secrets; Trademarks</li>
</ol>
</div>
* EE 546: Mathematics of High-Dimensional Data (Fall 2018)  
<a href="javascript:showOrHide('syl_ee546');">Outline</a>, [Final Project](/work/classes/projects/proj_nf_ee546.pdf)
<div style="display:none" id="syl_ee546">
<ol class="listing-grey" style="margin-left: 4%;">
  <li>Introduction to Mathematics of Data; Sample Applications; Optimization Basics</li>
  <li>Optimization for Modern Data Analysis I: First Order Methods, Accelerated Schemes</li>
  <li>Optimization for Modern Data Analysis II: Sub-Gradients and Non-Smooth Optimization, Incremental and Stochastic Schemes</li>
  <li>Basics of Concentration of Measure and High Dimensional Probability</li>
  <li>Non-Asymptotic Random Matrix Theory and Matrix Concentration</li>
  <li>Dimension Reduction, Sketching, and Applications</li>
  <li>Fast and Randomized Methods for Numerical Linear Algebra</li>
  <li>Clustering I: Matrix Perturbation Theory</li>
  <li>Clustering II: Spectral Algorithms, Application in Community Detection</li>
  <li>Linear Inverse Problems I: Compressive Sensing and Sparsity, Recommender Systems, Matrix Completion and Low-Rank Modeling</li>
  <li>Linear Inverse Problems II: Recovery of ne-scale Data from coarse-scale Measurements: Applications in Deblurring, Fluorescence Microscopy, Wireless Communications, Medical Imaging and Computer Vision</li>
  <li>Modern Theory of Linear Inverse Problems; Iterative Algorithms and Non-Convex Optimization; Phase Retrieval and Computational Imaging</li>
  <li>Discrete and Submodular Optimization and Learning</li>
  <li>Learning Representations; Sparse Coding; Word Embeddings</li>
  <li>Kernel Methods; “Shallow" and “Deep" Learning</li>
</ol>
</div>
* EE 660: Machine Learning from Signals: Foundations and Methods (Fall 2017)  
<a href="javascript:showOrHide('syl_ee660');">Outline</a>, [Final Project](/work/classes/projects/proj_nf_ee660.pdf)
<div style="display:none" id="syl_ee660">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Introduction to Machine Learning</li>
    <li>Key Issues and Concepts in Machine Learning</li>
    <li>Multidimensional Regression: Linear Regression, Maximum-Likelihood and MAP Estimation, Ridge Regression, Bayesian regression; Learning Linear and Non-Linear Relationships</li>
    <li>Review of Convexity and Optimization</li>
    <li>Logistic Regression</li>
    <li>Feasibility of Learning: Deterministic and Statistical Views; Hoeffding Inequality; Inductive Bias</li>
    <li>Complexity of Learning I: Generalization; Estimation of Error on New Data; Implications in Dataset Usage</li>
    <li>Complexity of Learning II: Bias-Variance Decomposition; Learning Curves; Overfitting</li>
    <li>Regularization; Feature Reduction; Sparsity</li>
    <li>Model Selection and Validation</li>
    <li>Boosting Techniques and Decision Trees</li>
    <li>Kernel Methods</li>
    <li>Semi-Supervised Learning for Classification</li>
    <li>Unsupervised Learning for Clustering: Statistical Techniques</li>
    <li>Unsupervised Learning for Clustering: Other Techniques</li>
</ol>
</div>
* EE 559: Mathematical Pattern Recognition (Spring 2017)  
<a href="javascript:showOrHide('syl_ee559');">Outline</a>, [Final Project](/work/classes/projects/proj_nf_ee559.pdf)
<div style="display:none" id="syl_ee559">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Basic Concepts in Pattern Recognition; A Paradigm in Pattern Recognition</li>
    <li>Distribution-Free Classification I: Classifier Design; Discriminant Functions</li>
    <li>Distribution-Free Classification II: Training and Optimization for Supervised Learning; Perceptron; Support Vector Machines</li>
    <li>Statistical Classification I: Statistics are Known - Bayes Decision Theory</li>
    <li>Statistical Classification II: Statistics are Partially Known - Parameter Estimation: Maximum Likelihood, Maximum A Posteriori, Bayesian Estimation</li>
    <li>Statistical Classification III: Statistics are Unknown - Non-Parametric Techniques: Histogram, Parzen Windows, k-Nearest Neighbors</li>
    <li>Statistical Classification IV: Supervised Learning</li>
    <li>Validation and Cross-Validation; Feature Selection and Reduction</li>
    <li>Artificial Neural Networks</li>
</ol>
</div>
* EE 517: Statistics for Engineers (Spring 2017)  
<a href="javascript:showOrHide('syl_ee517');">Outline</a>, [Final Project](/work/classes/projects/proj_nf_ks_ee517.pdf)
<div style="display:none" id="syl_ee517">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Overview of Statistics; Probability Review</li>
    <li>Sampling Distributions</li>
    <li>Point Estimation</li>
    <li>Confidence Intervals</li>
    <li>Hypothesis Testing</li>
    <li>Tests for Probability Densities; Contingency Tables</li>
    <li>Sufficient Statistics; Cramer-Rao Bound; Ratio Hypothesis Tests</li>
    <li>Sequential Tests; Linear Regression; Heteroscedasticity</li>
    <li>Multiple Regression; Multicollinearity Diagnostics</li>
    <li>Model Building; Stepwise Regression; Statistical Process Control</li>
    <li>Other Regression Types; ANOVA</li>
    <li>Runs; Experimental Design; Bayesian Statistics</li>
    <li>Expectation-Maximization; Hierarchical Bayes and Gibbs Samplers; Non-Parametric/Robust Tools</li>
</ol>
</div>
* EE 503: Probability for Electrical and Computer Engineers (Fall 2016)  
<a href="javascript:showOrHide('syl_ee503');">Outline</a>
<div style="display:none" id="syl_ee503">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Logic and Sets; Sigma Algebras; Probability Axioms</li>
    <li>Independence; Total Probability; Bayes' Theorem</li>
    <li>Combinatorics; Binomial Theorem; Limits of Sequences</li>
    <li>Poisson Theorem; Negative Binomial; Formal Reasoning</li>
    <li>Random Variables; Densities and Cumulative Distributions</li>
    <li>Expectations and Moments of Random Variables</li>
    <li>Covariance; Correlation; Uncertainty Principles</li>
    <li>Stochastic Convergence; Laws of Large Numbers</li>
    <li>Conditional Expectations; Maximum Likelihood Estimation</li>
    <li>Transformed Densities; Random Sampling; Entropy</li>
    <li>Central Limit Theorem and Applications; Confidence Intervals</li>
    <li>Financial Engineering; Introduction to Martingales and Markov Chains</li>
    <li>Markov Chains: Estimation</li>
    <li>Markov Chains and Queues: Advanced Applications</li>
</ol>
</div>
* EE 562: Random Processes in Engineering (Fall 2016)  
<a href="javascript:showOrHide('syl_ee562');">Outline</a>
<div style="display:none" id="syl_ee562">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Definition of Random Processes: Random Variables, Random Vectors, Random Sequences, Random Waveforms, etc.</li>
    <li>Second Order Statistics: Properties of Correlation Functions</li>
    <li>Covariance Matrix Factorization; Eigenvalues - Eigenvectors; Causal Factoring and Whitening Concepts</li>
    <li>Gaussian Processes</li>
    <li>Simple Hypothesis Tests</li>
    <li>Linear Minimum-Mean-Square-Error Estimation; Orthogonality Principle</li>
    <li>Linear Operations on Random Processes; Convergence Concepts: Convolution, Integration, Differentiation</li>
    <li>Frequency Domain Analysis: Time Invariant Linear Operations</li>
    <li>Energy Spectra; Power Spectra; White Noise Approximations</li>
    <li>Linear Transformations of Wide-Sense Stationary Random Processes; Spectral Factorization; Applications</li>
    <li>Poisson Distributed Events in Time; Campbell’s Theorem</li>
    <li>Karhuenen-Loeve Expansions of Finite Intervals</li>
    <li>Narrowband Process Representations</li>
    <li>Time Averages; Ergodicity</li>
</ol>
</div>

### Computer Science
* CSCI 662: Advanced Natural Language Processing (Fall 2020)  
<a href="javascript:showOrHide('syl_csci662');">Outline</a>
<div style="display:none" id="syl_csci662">
<ol class="listing-grey" style="margin-left: 4%;">
  <li>Introduction and Probability Basics</li>
  <li>Linear Classifiers: Naive Bayes, Logistic Regression, Perceptron</li>
  <li>Nonlinear Classifiers, feed-forward neural networs, backpropagation, gradient descent</li>
  <li>POS tags, HMMs, search</li>
  <li>Parsing and Syntax I: treebanks, evaluation, CKY, grammar induction, PCFGs</li>
  <li>Parsing and Syntax II: dependencies, shift-reduce</li>
  <li>Evaluation, Annotation, Mechanical Turk</li>
  <li>Semantics: Word sense, PropBank, AMR, distributional and lexical semantics</li>
  <li>Language Models: n-gram, feed-forward, recurrent</li>
  <li>Machine Translation history, evaluation, statistical MT</li>
  <li>Neural Machine Translation, summarization, generation</li>
  <li>Transformers</li>
  <li>Large Contextualized Language Models (ElMo, BERT, GPT, etc.)</li>
  <li>Information Extraction: Entity/Relation, CRF, Events, zero-shot</li>
  <li>Blade Runner NLP / Bertology</li>
  <li>Creative generation, structure-to-text, text-to-text</li>
  <li>Dialogue</li>
  <li>Power and Ethics</li>
</ol>
</div>
* CSCI 626: Computational Social Sciences: Text as Data (Fall 2019)  
<a href="javascript:showOrHide('syl_csci626');">Outline</a>, [Final Project](/work/classes/projects/proj_ta_sc_nf_aj_csci626.pdf)
<div style="display:none" id="syl_csci626">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Introduction to Computational Social Sciences</li>
    <li>Dictionary Methods</li>
    <li>Differential Language Analysis</li>
    <li>Latent Semantic Analysis</li>
    <li>Neural Networks</li>
    <li>Clinical and Cognitive Applications</li>
    <li>Ethics</li>
</ol>
</div>
* CSCI 599: Machine Learning Theory (Spring 2018)  
<a href="javascript:showOrHide('syl_csci599b');">Outline</a>, [Final Project](/work/classes/projects/proj_nf_csci599b.pdf)
<div style="display:none" id="syl_csci599b">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Introduction to Machine Learning Theory; Supervised Learning</li>
    <li>Online Learning: Winnow, Best Experts, Weighted Majority, Perceptron Algorithms</li>
    <li>Generic Bounds for Online Learning</li>
    <li>VC dimension and Sample Complexity</li>
    <li>PAC Learning Model; Online to PAC Conversion; Occam's Razor</li>
    <li>VC Dimension and Sample Complexity of PAC Learning</li>
    <li>Weak and Strong Learning</li>
    <li>Boosting</li>
    <li>PAC Learning with Noise: Random Classification Noise, Malicious Noise</li>
    <li>Statistical Query (SQ) Learning; Simulating SQ Queries in the Presence of Random Classification Noise</li>
    <li>Adaboost Algorithm and Analysis</li>
</ol>
</div>
* CSCI 599: Deep Learning and its Applications (Fall 2017)  
 <a href="javascript:showOrHide('syl_csci599');">Outline</a>, [Final Project](https://nikosfl.github.io/sc-scd/)
<div style="display:none" id="syl_csci599">
<ol class="listing-grey" style="margin-left: 4%;">
    <li>Machine Learning Review</li>
    <li>Loss Functions and Optimization; Feed Forward Neural Networks</li>
    <li>Convolutional Neural Networks (CNNs)</li>
    <li>Training Neural Networks</li>
    <li>CNN Architectures</li>
    <li>Deep Learning Software</li>
    <li>Recurrent Neural Networks</li>
    <li>Generative Adversarial Networks (GANs)</li>
    <li>Variational Autoencoders</li>
    <li>PixelRNN; PixelCNN</li>
    <li>Deep Reinforcement Learning</li>
    <li>InfoGAN; CycleGAN</li>
    <li>Attention Networks; Relational Networks; Memory Networks</li>
    <li>AlphaGo; AlphaGo Zero</li>
    <li>Imitation Learning</li>
</ol>
</div>

## National Technical University of Athens
*I had to complete 61 classes before obtaining my Diploma; here I am only listing a subset of those I took after my 6th semester (when I had to choose my flows of specialization) and are relevant to Signal Processing, Artificial Intelligence, or Software Development.* 
* Pattern Recognition with Emphasis on Speech Recognition 
* Speech and Natural Language Processing 
* Computer Vision 
* Image and Video Analysis and Technology
* Digital Signal Processing
* Neural Networks and Intelligent Systems 
* Knowledge Systems and Technologies 
* Artificial Intelligence  
* Software Engineering 
* Algorithms and Complexity
* Operating Systems 
* Databases 
* Programming Languages I
<!-- * Physiological Systems Modeling, Simulation, and Control * Optimization Techniques and Control Applications * Graph Theory -->
<!-- * Applied Mathematics - Calculus of Variations * Biomedical Technology Laboratory * Electromagnetic Compatibility -->
<!-- * Mathematical Logic for Computer Science * Control Systems Design -->

## Massive Online Open Courses
*offered by Coursera*
* Bioinformatics Algorithms - Part I (Feb 2014)
* Neuroethics (Nov 2013)
* Neural Networks for Machine Learning (Nov 2012)
* Computing for Data Analysis (Oct 2012)  

*offered by Udacity*
* Artificial Intelligence for Robotics (Apr 2012)
* Introduction to Computer Science (Apr 2012)  

*offered by Stanford University*
* Machine Learning (Dec 2011)  
* Introduction to Artificial Intelligence (Dec 2011) 
* Introduction to Databases (Dec 2011) 

<!--[[Syllabus](/work/classes/syllabus/syl_ee546.pdf)]-->  
<!--[[Syllabus](/work/classes/syllabus/syl_ee660.pdf)], [[Final Project](/work/classes/projects/proj_nf_ee660.pdf)]-->
<!-- [[Syllabus](/work/classes/syllabus/syl_ee559.pdf)], [[Final Project](/work/classes/projects/proj_nf_ee559.pdf)] -->
<!-- [[Syllabus](/work/classes/syllabus/syl_ee517.pdf)], [[Final Project](/work/classes/projects/proj_nf_ee517.pdf)] -->
<!-- [[Syllabus](/work/classes/syllabus/syl_ee503.pdf)] -->
<!-- [[Syllabus](/work/classes/syllabus/syl_ee562.pdf)] -->
<!-- [[Syllabus](http://www.iliasdiakonikolas.org/teaching/Spring18/CSCI599.html)], [[Final Project](/work/classes/projects/proj_nf_csci599b.pdf)] -->
<!-- [[Syllabus](/work/classes/syllabus/syl_ee599.pdf)], [[Final Project](https://nikosfl.github.io/sc-scd/)] -->
